---
title: "Main notebook"
output: pdf_document
date: '2023-02-01'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Acknowledgements

The data and a large portion of data preprocessing is provided by Jane Reid. The re-implementation into INLA is also largely based on the work from Stefanie Muff.

# Data loading

```{r data loading}
library(MCMCglmm)
library(MASS)
library(nadiv)
library(bdsmatrix)
library(INLA)
library(QGglmm)
# library(patchwork)
# library(SMisc)
library(ggplot2)
library(latex2exp)
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
if (!require("GeneticsPed", quietly = TRUE))
    BiocManager::install("GeneticsPed")
if (!require("MCMCglmm", quietly = TRUE))
    install.packages("../MCMCglmm-rbv-patch.tar.gz")

library("GeneticsPed") 
library("MCMCglmm")


qg.data.gg.inds <- read.table("../data/qg.data.gg.inds.steffi.txt", header=T)
d.ped <- ped.prune.inds <- read.table("../data/ped.prune.inds.steffi.txt", header=T)
d.Q <-  read.table("../data/Q.data.steffi.txt", header=T)

qg.data.gg.inds$natalyr.id <- qg.data.gg.inds$natalyr.no

```

Below we do a couple more preprocessing steps

```{r}
# Scale the continuous variances for stability
qg.data.gg.inds$f.coef.sc <- scale(qg.data.gg.inds$f.coef,scale=FALSE)
qg.data.gg.inds$g1.sc <- scale(qg.data.gg.inds$g1,scale=FALSE)
qg.data.gg.inds$natalyr.no.sc <- scale(qg.data.gg.inds$natalyr.no,scale=FALSE)
qg.data.gg.inds$brood.date.sc <- scale(qg.data.gg.inds$brood.date,scale=FALSE)

# Binarize `sex` covariate
qg.data.gg.inds$sex <- qg.data.gg.inds$sex.use.x1 - 1 
```

## Deriving *A*

For INLA we need ids that run from 1 to the number of individuals

```{r}
d.ped <- nadiv::prepPed(d.ped)
d.ped$id <- 1:(nrow(d.ped))

# Maps to keep track of the Ninecode to ID relations
d.map <- d.ped[,c("ninecode","id")]
d.map$g1 <- d.Q[match(d.map$ninecode,d.Q$ninecode),"g1"]
d.map$foc0 <- d.Q[match(d.map$ninecode,d.Q$ninecode),"foc0"]

# Give mother and father the id
d.ped$mother.id <- d.map[match(d.ped$gendam, d.map$ninecode),"id"]
d.ped$father.id <- d.map[match(d.ped$gensire, d.map$ninecode),"id"]

# A can finally be constructed using `nadiv`
Cmatrix <- nadiv::makeAinv(d.ped[,c("id","mother.id","father.id")])$Ainv

# Stores ID twice (to allow for extra IID random effect)

qg.data.gg.inds$id <- d.map[match(qg.data.gg.inds$ninecode, d.map$ninecode), "id"]
qg.data.gg.inds$u <- 1:nrow(qg.data.gg.inds) # Extra IID effect

```


# INLA

The general INLA formula is provided below, where `f()` encode the random effect:

```{r}
FORMULA_EXTRA_IID_NOISE = FALSE # Change this to include iid N(0,1) noise

formula.inla.scaled = surv.ind.to.ad ~ f.coef.sc + g1.sc + natalyr.no.sc + brood.date.sc + sex +
  f(nestrec, model="iid",hyper=list(
    prec=list(initial=log(1/0.05), prior="pc.prec",param=c(1,0.05)) # PC priors
  )) +
  f(natalyr.id, model="iid",hyper=list(
    prec=list(initial=log(1/0.25), prior="pc.prec",param=c(1,0.05)) # PC priors
  )) +
  f(id,model="generic0", # Here we need to specify the covariance matrix 
    Cmatrix=Cmatrix,     #    via the inverse (Cmatrix)
    constr = F, # Doesn't really matter
    hyper=list(
      prec=list(initial=log(1/10), prior="pc.prec",param=c(1,0.05)) # PC priors
     )) 
if(FORMULA_EXTRA_IID_NOISE){
  formula.inla.scaled = update(formula.inla.scaled,
                               ~ . + f(u, model="iid", constr=T,
                                       hyper=list(prec=list(
                                         initial=log(1),
                                         fixed=T))
                                       )
                               )
}
```

Now we call INLA models (this takes some time). Note that we pass some control arguments to the function call. We compute DIC (Deviance information criterion) for all models with `dic` flag in `control.compute`.  For the binomial models, we want to be able to use `QGglmm` and average over all fixed effects. This is done by supplying the *latent marginal predicted values*, which are not computed unless you pass the `return.marginals.predictor` flag set to true. We also want to set the `CPO` flag to true in the Gaussian model, so we can look a bit at its "residuals" (PIT values). Lastly, the `control.family` argument is used to pass the link functions for binomial models.

```{r}

fit.inla.probit = inla(formula=formula.inla.scaled, family="binomial",
                       data=qg.data.gg.inds,
                       control.compute=list(dic=T, return.marginals.predictor=T),
                       control.family = list(link = "probit"),
)

fit.inla.logit = inla(formula=formula.inla.scaled, family="binomial",
                      data=qg.data.gg.inds,
                      control.compute=list(dic=T, return.marginals.predictor=T),
                      control.family = list(link = "logit"),
)

fit.inla.gaussian = inla(formula=formula.inla.scaled, family="gaussian",
                             data=qg.data.gg.inds,
                             control.compute=list(dic=T, cpo=T) 
)

data.frame(Gaussian = fit.inla.gaussian$dic$dic, Logit = fit.inla.logit$dic$dic,
           Probit = fit.inla.probit$dic$dic, row.names="Deviance Information Criteria")
```




## Residual analyses on Gaussian model

For the Gaussian INLA model we also compute PIT (probability integral transform) values. They resemble the probability that a new response is less than the observed response. Under Gaussian model assumptions, PIT values should be uniformly distributed. [Source](https://julianfaraway.github.io/brinla/examples/chicago.html).


* The first plot are the sorted PIT values over quantiles, analogous to a Q-Q plot in frequentist data. It shows a clear non-linear trend but rather a sigmoid-like curve.
* The second plot shows the PIT values across the different posterior fitted value means. Here we expect no clear pattern for well-behaved models, which is not the case in our model.
* The third plot is the residuals $y_i - \hat{y_i}$ with 95% credible interval. Here, we see a clear separation of those

```{r gaussian plotting}
SAVE.PLOT = TRUE # global setting

pit.g = fit.inla.gaussian$cpo$pit # PIT-values

# <Plot 1> Analogous to QQ-plot so should be linear
# --------
ggplot(data=data.frame(
  Quantiles=1:length(pit.g)/(length(pit.g)+1), PIT=sort(pit.g))) +
  geom_point(aes(x=Quantiles, y=PIT)) + ggtitle("Sorted PIT values for Gaussian model")
if(SAVE.PLOT) ggsave("../figures/PIT-sorted.pdf")

# <Plot 2> Posterior mean fitted values as a function of PIT values
# --------    analagous to "Residuals vs fitted"

ggplot(cbind(fit.inla.gaussian$summary.fitted.values, pit.g),
       aes(x=mean, y=pit.g)) +
  geom_point() +
  geom_smooth() +
  labs(title="PIT values over posterior mean fitted values",
                      x="Posterior fitted values (mean)",
                      y="PIT value")
if(SAVE.PLOT) ggsave("../figures/PIT-over-fitted.pdf")


# <Plot 3> Mean linear predictor of different models, showing 
# --------    how different predictors for the models are. Not in use.
plot.inla.linear.predictors <- function(){
  par(mfrow=c(1,3))
  plot(fit.inla.logit$summary.linear.predictor[, 1],
       fit.inla.probit$summary.linear.predictor[, 1],
       main="\nLogit (x) versus Probit (y)", xlab="",ylab=""
  )
  abline(0,1)
  plot(fit.inla.gaussian$summary.linear.predictor[, 1],
       fit.inla.probit$summary.linear.predictor[, 1],
       main="\nGaussian (x) versus Probit (y)", xlab="", ylab=""
  )
  abline(0,1)
  plot(fit.inla.gaussian$summary.linear.predictor[, 1],
       fit.inla.logit$summary.linear.predictor[, 1],
       main="\nGaussian (x) versus Probit (y)", xlab="", ylab=""
  )
  abline(0,1)
  mtext("Mean linear predictor of the different models",
        side = 3, line = -1, outer = TRUE)
}


# <Plot 4> Plot of 'residuals', i.e. difference in true data and the 
# --------    mean of the fitted values
df.resid = qg.data.gg.inds$surv.ind.to.ad - 
  fit.inla.gaussian$summary.fitted.values
rownames(df.resid) = 1:nrow(df.resid)
df.resid$class = qg.data.gg.inds$surv.ind.to.ad

ggplot(data=df.resid, aes(x=as.numeric(row.names(df.resid)), y=mean, color=factor(class))) +
  geom_errorbar(aes(ymin=`0.025quant`, ymax=`0.975quant`), color="darkgrey") +
  geom_point() + scale_color_manual(name="Juvenile survival", values=c("darkred", "steelblue")) +
  labs(title="Residuals of Gaussian model", x="Index", y="Residuals")
if(SAVE.PLOT) ggsave("../figures/Residuals-gaussian.pdf")


# <Plot 5> Recreation of QQ plot using residuals computed above instead of PIT values
#           Unused
plot.inla.qqplot <- function(){
  n.obs = length(qg.data.gg.inds$u)
qqplot(qnorm(ppoints(n.obs),
             mean = mean(qg.data.gg.inds$surv.ind.to.ad),
             sd = sd(qg.data.gg.inds$surv.ind.to.ad)
             ),
       resids, xlab="Theoretical quantiles", ylab="Sample Quantiles",
       main="Q-Q plot from 'residuals' above")  
}

```
## Transformations of heritability

Before developing methods for transformed heritability, we need to be able to sample from the marginal fitted values on latent scale.

```{r}

marginal.latent.mode <- function(fit){
  #' Get mode for each marginal linear predictor
  modes = c()
  iter = 1
  for(predictor in names(fit$marginals.linear.predictor)){
    xy = get(predictor, fit$marginals.linear.predictor)
    modes[iter] = xy[, "x"][which.max(xy[, "y"])]
    iter = iter + 1
  }
  modes
}


marginal.latent.samples <- function(fit, nsamples){
  #' Rather than only using mode for each predictor, we use samples
  #' from its posterior, with _nsamples_ samples. 
  #' Output is list of _nsamples_ elements, with each element in the list being
  #' a vector of the predictor size (i.e. number of observations in data)
  out_transpose = matrix(nrow=nsamples,
                         ncol=length(fit$marginals.linear.predictor))
  for(i in seq_along(fit$marginals.linear.predictor)){
    xy = get(names(fit$marginals.linear.predictor)[i], fit$marginals.linear.predictor)
    out_transpose[, i] = inla.rmarginal(nsamples, xy)
  }

  # We want list where each list element is one sample, i.e. the transposed
  out = list()
  for(i in 1:nsamples){
    out[[i]] = out_transpose[i, ]
  }
  out
}


report.max.skewness <- function(posterior){
  #' Made esepcially for marginal linear predictor but should work on any
  #' list containing named "x" and "y" columns
  library(e1071)
  iter = 1
  posterior_skews = c()
  for(predictor in names(posterior)){
    posterior_skews[iter] = skewness(get(predictor, posterior)[, "x"])
    iter = iter + 1
  }
  cat("Minimum skew for list no.", which.min(posterior_skews), "with skewness",
      min(posterior_skews), "and max for list no.", which.max(posterior_skews),
      "with skewness", max(posterior_skews), ".\n")
}


```

We can now define methods to obtain heritability on the different scales. The first function computes $h^2$ on latent scale, or using the direct transformation by including link variance in denominator. The second method is more comprehensive and uses the library `QGglmm` to obtain estimates on data scale.

```{r}

get.h2 <- function(inla.fit, n, use.scale=F, model=NA){
  #' Get n samples of heritability from INLA fit
  #' h^2 computed as reciprocal of precision for id, over the sum of 
  #' (the reciprocal of) all random effects / hyperparameters precisions
  #' Input: INLA object, number of samples, scaling flag and model parameter.
  #' Scaling flag determines if we want latent h^2 (F) or observation-scale,
  #' including the link variance in the binomial models.
  samples <- inla.hyperpar.sample(n=n,inla.fit)
  denominator = 0
  for(cname in colnames(samples)){ denominator = denominator + 1/samples[, cname]}
  
  if(use.scale){
    scales.dictionary = list(binom1.probit=1, binom1.logit=pi^2/3, round=0.25)
    scale.param = get(model, scales.dictionary)
    denominator = denominator + scale.param
  }
  
  h2.inla <- (1/samples[,"Precision for id"]) / denominator
  return(h2.inla)
}

get.h2.from.qgparams <- function(inla.fit, 
                                 modelname, 
                                 n, 
                                 averaging=F,
                                 averaging.mode.only=F){
  #' Computes a posterior of data-scale heritability (h2) using QGParams
  #'
  #' Params:
  #' inla.fit:  The fitted INLA object
  #' modelname: A string specifying model
  #'    ("Gaussian", "binomN1.probit", "binom1.logit")
  #' n:         Number of samples  
  #' averaging: Flag to determine if we want to average over fixed effects
  #' mode.only: Flag to determine if marginal latent should just be mode

  stopifnot(modelname %in% c("Gaussian", "binom1.probit", "binom1.logit"))
  samples.posterior <- inla.hyperpar.sample(n=n,inla.fit)
  vp.samples = 0
  for(cname in colnames(samples.posterior)){
    vp.samples = vp.samples + 1/samples.posterior[, cname]
  }
  
  if(! averaging){
    mu = inla.fit$summary.fixed$mean[1] # Intercept
    va.samples = 1/samples.posterior[,"Precision for id"]
    kwargs = list(verbose=F)
  
    h2.getter = function(...){
      get("h2.obs", suppressWarnings(QGparams(...)))
      }
    posterior = mapply(h2.getter, mu, va.samples, vp.samples, modelname, MoreArgs=kwargs)
  }
  else{
    # Average over fixed effects
    debug_mu = fit$summary.fixed$mean[1]
    vp.samples = 0
    df <- data.frame(va = as.vector(1/posterior.samples[, "Precision for id"]),
                     vp = as.vector(vp.samples)
                     )
    if(! averaging.mode.only){
      df$predict = marginal.latent.samples(fit, nsamples)
      posterior = do.call("rbind", apply(df, 1, function(row){
        QGparams(predict=row[["predict"]], var.a=row[["va"]], var.p=row[["vp"]],
                 model=modelname, verbose=F)
      }))
    }
    else{
      predict.argument = marginal.latent.mode(inla.fit)
      posterior = do.call("rbind", apply(df, 1, function(r){
        QGparams(predict=predict.argument, var.a=row[["va"]], var.p=row[["vp"]],
                 model=modelname, verbose=F)
      }))
    }
  }
  
  return(posterior)
}

#### Log from running function on Markov ####
# Called function at: Mon Feb 27 11:39:41 AM 2023
# Entering `new.h2.transf` (Bayesian sampling)
# DEBUG: Entering sampling for loop
# DEBUG: Entering transposing loop
# Done after 115.9634 minutes. Entering same function without the Bayesian stuff.
# Done after 106.1235 minutes. Now without averaging:Done after 0.1312247 minutes.
# Saving to disk...
# [1] 0
# Called function at: Mon Feb 27 03:21:54 PM 2023
# Entering `new.h2.transf` (Bayesian sampling)
# DEBUG: Entering sampling for loop
# DEBUG: Entering transposing loop
# Done after 4.129497 minutes. Entering same function without the Bayesian stuff.
# Done after 0.1096536 minutes. Now without averaging:Done after 0.05950377 minutes.
# Saving to disk...
# [1] 0
```

Now we compute the heritability on the different scales.

```{r}
# Compute h2
# TODO consider changing this to rather be stored in data.frame, this method is
#       difficult to handle..
n.samples = 10000
heritability = data.frame(gaussian.latent=get.h2(fit.inla.gaussian, n.samples))

heritability$logit.latent = get.h2(fit.inla.logit, n.samples)
heritability$logit.scaled = get.h2(fit.inla.logit, n.samples, use.scale=T,
                                  model="binom1.logit")
heritability$logit.qgglmm = get.h2.from.qgparams(fit.inla.logit,
                                                 "binom1.logit", n.samples)

heritability$probit.latent = get.h2(fit.inla.probit, n.samples)
heritability$probit.scaled = get.h2(fit.inla.probit, n.samples,use.scale=T,
                                   model="binom1.probit")
heritability$probit.qgglmm = get.h2.from.qgparams(fit.inla.probit,
                                                  "binom1.probit", n.samples)
```

Plotting some histograms, first the latent scale for the different binomial link functions.

```{r latent h2 for binomials}

df.latent.h2 = data.frame(
  samples=c(heritability$logit.latent, heritability$probit.latent),
  Model=c(rep("Logit",length(heritability$logit.latent)),
          rep("Probit",length(heritability$probit.latent))
          )
  )

ggplot(df.latent.h2, aes(x=samples, fill=Model)) +
  geom_density(alpha=0.5) +
  ggtitle("Posterior of latent scale heritability for binomial INLA models") +
  xlab("(Latent-scale) Heritability") +
  ylab("Density")
```
Generally we don't see much difference. Now we want to see how the gaussian one preforms.

```{r scaled h2 posteriors}
df.transformed.h2 = data.frame(
  samples=c(
    unname(heritability$logit.qgglmm),
    unname(heritability$probit.qgglmm),
    unname(heritability$gaussian.latent)
    ),
  Model=c(
    rep("Logit",length(heritability$logit.qgglmm)),
    rep("Probit", length(heritability$probit.qgglmm)),
    rep("Gaussian (no transformation)",length(heritability$gaussian.latent))
    )
  )

ggplot(df.transformed.h2, aes(x=samples, fill=Model)) +
  geom_density(alpha=0.5) +
  ggtitle("Posterior of transformed heritability") +
  xlab("Heritability") +
  ylab("Density") + xlim(c(0,0.1))#+ scale_x_log10()


```
It seems to capture around the same stuff but the tail in the Gaussian posterior is way larger.

So far we've only used `QGglmm` without averaging over fixed effects. This takes considerably more time to process, but we still do that one time to compare the results. Then we compare the heritability on four different scales
* Using 10k samples from the *marginal linear predictor*, and using this to average over
* Grabbing the mode for each *marginal linear predictor*, and passing the mode for each 10k sample
* No averaging, i.e. use intercept value instead.
* Use direct scaling method with link variance.

We see that the 3 first provide almost exactly the same result. Thus, we don't need to average over fixed effects in this dataset.

```{r qgparam setting exploration}
## analyze things ran on markov
plot.markov.result <- function(rdata.file, ssh.samples, plot.title=NA, saveplot=F){
  load(rdata.file)
  df = data.frame(
    Heritability=c(heritability_averaged$h2.obs,
                   heritability_frequentist_avged$h2.obs,
                   heritability_notavged,
                   scaled.heritability),
    Method = c(rep("Bayesian",ssh.samples),
               rep("Frequentist",ssh.samples),
               rep("No averaging",ssh.samples),
               rep("Direct scaling",ssh.samples)
               )
  )
  p <- ggplot(df, aes(x=Heritability, fill=Method)) + geom_density(alpha=0.5) +
    ylab("Density") #+ scale_x_log10()
  if(!is.na(plot.title)) p = p + labs(title=plot.title)
  if(saveplot){
    ggsave(filename = paste0("../figures/",
                      substring(rdata.file,1,nchar(rdata.file)-6),
                      ".pdf"),
           plot = p
    )
  }
  p
}
plot.markov.result("heritabilities_SSH_binom1.probit.Rdata", 10000,
                   plot.title="Heritability for Probit model", saveplot=F)
plot.markov.result("heritabilities_SSH_binom1.logit.Rdata", 10000,
                   plot.title="Heritability for Logit model", saveplot=F)

oldPlottingHeritability <- function(){
  #' Wrapper for old code
  par(mfrow=c(2,2))
  load("heritabilities_SSH_binom1.logit.Rdata")
  truehist(heritability_averaged$h2.obs, main="Bayesian",
           xlab=expression(h[obs]^2))
  truehist(heritability_frequentist_avged$h2.obs, main="Frequentist",
           xlab=expression(h[obs]^2))
  truehist(heritability_notavged, main="No averaging",
           xlab=expression(h[obs]^2))
  truehist(scaled.heritability, main="Direct scaling",
           xlab=expression(h[obs]^2))
  load("heritabilities_SSH_binom1.probit.Rdata")
  truehist(heritability_averaged$h2.obs, main="Bayesian",
           xlab=expression(h[obs]^2))
  truehist(heritability_frequentist_avged$h2.obs, main="Frequentist",
           xlab=expression(h[obs]^2))
  truehist(heritability_notavged, main="No averaging",
           xlab=expression(h[obs]^2))
  truehist(scaled.heritability, main="Direct scaling",
           xlab=expression(h[obs]^2))  
}

# TODO retrieve logs from laptop to find runtime!
```


Method to export heritability estimates in a TeX table

```{r}
get_mode <- function(vec){
  d = density(vec)
  d$x[which.max(d$y)]
}


print_one_metric <- function(fit, param, digits){
  paste(
    round(mean(get(param, fit)),digits), " (",
    round(get_mode(get(param, fit)),digits), ")\\newline $\\pm$ ",
    round(sd(get(param,fit)),digits), sep="")
}

print_heritability_table <- function(digits){
  #' `digits` is the number of significant digits
  #' Prints tabularx-table of heritability with posterior mean, posterior mode
  #' and standard deviation. Does it for logit, probit and Gaussian.
  header = paste( "% TABLE FROM R:", format(Sys.time(), "%a %b %d %X %Y"), "\n",
  "\\begin{table}[ht]\\centering\n",
    "\\begin{tabularx}{\\textwidth}{lXXX}\n",
    "\\hline\n",
    " & Binomial logit & Binomial probit & Gaussian  \\\\ \n",
    "\\hline \n"
  )
  main = paste(
    "Latent &", print_one_metric(heritability, "logit.latent", digits),
    "&", print_one_metric(heritability, "probit.latent", digits),
    "&", print_one_metric(heritability, "gaussian.latent", digits),
    "\\\\ \n",
    "Scaled &", print_one_metric(heritability, "logit.scaled", digits),
    "&", print_one_metric(heritability, "probit.scaled", digits),
    "& --",
    "\\\\ \n",
    "QGglmm &", print_one_metric(heritability, "logit.qgglmm", digits),
    "&", print_one_metric(heritability, "probit.qgglmm", digits),
    "& -- \n"
  )
  footer = paste("\\end{tabularx}",
               "\\caption{<Insert caption>}",
               "\\label{tab:heritabily gutta}",
               "\\end{table}", sep="\n")

  cat(header, main, footer,sep="\n")
}
print_heritability_table(4)

```

# Simulation data

```{r}

simulated.heritability <- function(NeNc=0.5, idgen=100, nGen = 9, sigmaA=0.8,
                                   linear.predictor=NA, simulated.formula=NA,
                                   dichotomize="round"){
  #' Generate pedigree, fit gaussian (INLA) model and 
  #' Remark: linear.predictor should be a callable and pass (at least) u
  #' 
  #' Input:
  #' NeNc:    Effective/Census population mean, used to determine
  #'          Number of fathers and mothers per generation,
  #' idgen:   Numer of individuals per generation in pedigree
  #' nGen:    Number of generations in pedigree
  #' sigmaA:  Additive genetic variance (to get breeding values)
  #' 
  #' linear.predictor:  Callable function of 'u' (breeding values),
  #'                    Should be centered around 0
  #' simulated.formula: Formula expression using the response name
  #'                    `simulated.response`, param `id` and `Cmatrix`,
  #'                    all of which are defined locally in this method.
  #' Output:
  #' heritability:  Posterior latent heritability samples (no transformation)
  #' summary:       List of mean and quantiles of posterior heritability
  #' p:             Portion of TRUE observations in simulated response
  
  ped0 <- generatePedigree(nId = idgen, nGeneration = nGen, 
                           nFather = idgen * NeNc, nMother = idgen * NeNc)
  # Set correct format for pedigree
  pedigree <- ped0[ , c(1,3,2)]
  names(pedigree) <- c("id", "dam", "sire")
  
  # Generate random breeding values
  # The following will CRASH if you don't use the patched MCMCglmm package!
  u <- rbv(pedigree, sigmaA)
  
  simulated.d.ped <- nadiv::prepPed(pedigree)
  simulated.Cmatrix <- nadiv::makeAinv(simulated.d.ped)$Ainv 
  
  # Generating "true" y_i
  
  if(dichotomize == "binom"){
      sigmoid.scale <- function(x) exp(x)/(exp(x)+1)
      simulated.response <- rbinom(length(u), size=1,
                                   prob=sigmoid.scale(linear.predictor(u)))
  }
  else if(dichotomize == "round"){
    # This assumes mean of \eta_i is 0
    simulated.response <- ifelse(linear.predictor(u) <= 0, 0, 1)
  }
  else{
    stop(paste0("Unknown dichotomization method '", dichotomize, "'. ",
                "Consider using 'binom' or 'round'."))
  }
  p = mean(simulated.response) # portion of true responses 

  
  # Model fitting LMM for binary trait
  # First reload formula environment to access local variables
  environment(simulated.formula) <- environment()
  simulated.fit.inla = inla(formula=simulated.formula, family="gaussian",
                             data=simulated.d.ped
                            )
  # Checks for error status in INLA fit,
  stopifnot(simulated.fit.inla$mode$mode.status == 0) # status != 0 is trouble
  heritability = get.h2(simulated.fit.inla, 10000) #10k samples should be suff
  list(
    heritability=heritability,
    summary=list(
      mean=mean(heritability), standard.deviation=sd(heritability),
       quantiles=quantile(heritability, probs=c(0.025, 0.5, 0.975))),
    p=p,
    simulated.response=simulated.response
  )
}


```

Now we try to run it through the model pipeline. Here we try $\eta_i = u_i + \varepsilon_i$ where residuals have variance $V_E=1$.

```{r}

threshold.scaling.param <- function(p){
  # h^2_l = threshold.scaling.param * h^2_obs
  p*(1-p)/(dnorm(qnorm(p)))^2
}

simulated.formula = simulated.response ~ f(id,model="generic0",
    Cmatrix=simulated.Cmatrix,
    constr = F, # Shouldn't matter
    hyper=list(
      prec=list(initial=log(1/10), prior="pc.prec",param=c(1,0.05)) # PC priors
    ))

v_a = 0.1
result = simulated.heritability(linear.predictor=function(u) u+rnorm(length(u)),
                       simulated.formula=simulated.formula, sigmaA=v_a)


threshold.scaled.h2 <- 1/threshold.scaling.param(result$p) * result$heritability 
simulation.h2.true <- v_a/(v_a+1) # Dependent on linear predictor, e.g. sigmaA/(sigmaA+1)

data.frame(Simulation=c(
    simulation.h2.true,
    mean(threshold.scaled.h2),
    paste0("(",
      paste(
        format(
          quantile(threshold.scaled.h2,
                   probs=c(0.025,0.975)),
          digits=4),
        collapse=", "),
      ")"
      ),
    mean(result$summary$mean)
  ),
  row.names = c("True h^2", "Estimated h^2_obs, mean",
                "95% Confidence interval",
                "Estimated latent mean")
)

# ------------------------------------------------------------------------------
#' It doesn't seem like it works so well for larger sigmaA's.
#' Let's quantitatively look into deviation:
plot.h2.deviation <- function(dichotomize="round",
                              title="True versus fitted heritability",
                              SAVE.PLOT=T, plot.fn=NA){
  # sigmaA.list <- c(1:10 %o% 10^(-3:3)) # Log scale between 10^-3 to 10^3
  sigmaA.list = seq(0.001, 0.26, by=0.01)
  deviation = c()
  estimates = c()
  true.vals = c()
  for(sigmaA in sigmaA.list){
    cat(">")
    result = simulated.heritability(
      dichotomize=dichotomize,
      linear.predictor=function(u) u+rnorm(length(u)),
      simulated.formula=simulated.formula, sigmaA=sigmaA)
    
    threshold.scaled.h2 <- 1/threshold.scaling.param(result$p) * result$heritability
    # True h^2 is dependent on linear predictor, i.e. sigmaA/(sigmaA+1) here
    simulation.h2.true <- sigmaA/(sigmaA+1)
    
    true.vals = c(true.vals, simulation.h2.true)
    estimates = c(estimates, mean(threshold.scaled.h2))
    deviation = c(deviation, simulation.h2.true-mean(threshold.scaled.h2))
  }
  res = data.frame(estimates=estimates, deviation=deviation,
             true.vals=true.vals,
             sigmaA=sigmaA.list)
  # Plotting
  p = ggplot(data=res, aes(x=sigmaA)) +
    geom_line(aes(y=true.vals,color="True h^2")) +
    geom_line(aes(y=estimates, color="Fitted h^2")) +
    xlab(TeX("$\\sigma^2$")) + ylab(TeX("$h^2$")) + 
    scale_x_continuous(trans='log10') +
    scale_color_manual(name=title,
      breaks=c("True h^2", "Fitted h^2"),
      values = c("True h^2"='darkred', "Fitted h^2"='steelblue')
      )
  if(SAVE.PLOT) ggsave(paste0("../figures/simulation_deviance_",
                              if(!is.na(plot.fn)) plot.fn, ".pdf"), p)
  return(list(res=res,p=p))
  }

temp = plot.h2.deviation(SAVE.PLOT=F)
true.with.dich.var = temp$res$sigmaA/(temp$res$sigmaA +1+0.25)
temp$res$true.vals.dich = true.with.dich.var
ggplot(data=temp$res, aes(x=sigmaA)) +
  geom_line(aes(y=true.vals, color="True")) +
  geom_line(aes(y=estimates,color="Estimated"))  +
  geom_line(aes(y=true.vals*0.25, color="True * dich. variance")) +
  scale_color_manual(name="Simulated heritability",
                     values=c('True'='darkred',
                              'Estimated'='steelblue',
                              'True * dich. variance'='darkgreen'))

plot.h2.deviation(dichotomize = "binom",
                  title="Fitted versus true h^2, binomial response",
                  plot.fn="binom")


simulation.hyperparameters <- function(NeNc_list=0.1, sigmaA_list=0.4){
  p = list()
  if(length(NeNc_list) > 1){
    cat("Running simulations for NeNc values, sigmaA is fixed at", sigmaA_list[1])
    for(NeNc in NeNc_list){
      simulation_results = simulated.heritability(
      NeNc=NeNc, idgen= 100, nGen = 10, sigmaA = sigmaA_list,
      linear.predictor = function(u) u+rnorm(length(u)),
      simulated.formula = simulated.formula
      )
      p[[paste0(NeNc)]] = simulation_results$heritability
    }
  }
  else{
    cat("Simulations for sigmaA, NeNc is fixed at", NeNc_list[1])
    for(sigmaA in sigmaA_list){
      simulation_results = simulated.heritability(
      NeNc=NeNc_list, idgen= 100, nGen = 10, sigmaA = sigmaA,
      linear.predictor = function(u) u+rnorm(length(u)),
      simulated.formula = simulated.formula
      )
      p[[paste0(sigmaA)]] = simulation_results$heritability
    }
  }
  return(p)
}

p = simulation.hyperparameters(NeNc_list = c(0.05, 0.1, 0.25, 0.5))
p2 = simulation.hyperparameters(sigmaA_list=c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75)*100)

# TODO these are currently not saved as figured. Not sure if we need them.

par(mfrow=c(2,2))
for(i in names(p)){
  truehist(get(i,p), xlab=paste("NeNc =",i))
  abline(v=0.4)
}
par(mfrow=c(3,2))
for(i in names(p2)){
  truehist(get(i,p2), xlab = paste("sigmaA =", i))
  cat("True h2", as.numeric(i)/(as.numeric(i)+1), "\n")
  abline(v=as.numeric(i)/(as.numeric(i)+1)) # a+e_i, e_i~N
}

```
We want to research why this is so bad a little more. First, let's look at variances of the simulated $y_i$'s themselves.

```{r}
# Debugging poor simulation behavior
var(result$simulated.response) # Variance about 1/4

# Try to re-run with another linear predictor
report.simulated.threshold.estimate <- function(NeNc,
                                                idgen,
                                                nGen,
                                                sigmaA,
                                                linear.predictor,
                                                simulated.formula,
                                                Vp=NA){
  stopifnot("Vp required to get true h^2" = !is.na(Vp))
  sim.result = simulated.heritability(NeNc,idgen,nGen,sigmaA,
                                  linear.predictor,simulated.formula)
  threshold.scaled.h2 <- 1/threshold.scaling.param(sim.result$p) * 
    sim.result$heritability
  simulation.h2.true <- sigmaA/(Vp)
  
  data.frame(Simulation=c(
    simulation.h2.true,
    mean(threshold.scaled.h2),
    paste0("(",
      paste(
        format(
          quantile(threshold.scaled.h2,
                   probs=c(0.025,0.975)),
          digits=4),
        collapse=", "),
      ")"
      ),
    mean(sim.result$summary$mean)
  ),
  row.names = c("True h^2", "Estimated h^2_obs, mean",
                "95% Confidence interval",
                "Estimated latent mean")
)
}
report.simulated.threshold.estimate(0.5, 100, 24, 10, function(u) u,
                                    simulated.formula,10)

sim.result = simulated.heritability(NeNc=0.5,idgen=100,nGen=9,sigmaA=10,
                                    linear.predictor = function(u) u,
                                    simulated.formula = simulated.formula)



# Now we want to look into how A (relatedness matrix) looks like..
plot.A.matrix <- function(pedigree, title.append=NA){
  A.matrix = nadiv::makeA(pedigree)
  A.diag = diag(A.matrix)
  A.nondiag = A.matrix
  diag(A.nondiag) = NA
  {
  par(mfrow=c(2,1))
  truehist(A.nondiag@x,
           main=paste0(
             "Off-diagonal elements of A",
             if(!is.na(title.append)) title.append
             ),
           xlab="Relatedness")
  plot(A.diag, main="Diagonal elements of A")
  }
}

# For the simulation data:
ped0 <- generatePedigree(nId = 100, nGeneration = 24, 
                         nFather = 0.5*100, nMother = 0.5*100)
pedigree <- ped0[ , c(1,3,2)]
names(pedigree) <- c("id", "dam", "sire")
simulated.d.ped <- nadiv::prepPed(pedigree)
plot.A.matrix(simulated.d.ped, title.append = ", 24 generation simulation")


# Song sparrow data
plot.A.matrix(d.ped[,c("id","mother.id","father.id")], ", Song sparrow data")

```
This is an interesting observation. First, most of the off-diagonal elements are very close to 0, indicating almost no relatedness. Secondly, not all diagonal elements are themselves 1. What does this mean?

Finally, let's try to generate the response differently,


## Dichotomization variance

In general $u \sim \mathcal N(0, \sigma^2_A)$. Let $\eta=u$. And let $v$ be defined so that
$$
v = 
\begin{cases}
0, \quad u\le0,\\
1,\quad u > 0
\end{cases}
$$
Then,
$$
E[v] = 0 \cdot P(v=0) + 1\cdot P(v=1) = P(u>0) = 1-P(u/\sigma_A \le0)=1-\Phi(0)=0.5
$$
The variance of $v$ becomes
$$
Var(v)=E[\left(v-E(v)\right)^2]=P(v=0)(0-0.5)^2+P(v=1)(1-0.5)^2=0.25\;\forall \sigma^2_A
$$

$$
Var(v)/Var(u)=\frac{1}{4\sigma^2}
$$

Similarly, if $\eta=u+z, z\sim\mathcal N(0,1)$, then
$$
v = 
\begin{cases}
0, \quad u+z\le0,\\
1,\quad u+z > 0
\end{cases}
$$
Similarly, since $u+z \sim \mathcal N(0,\sigma^2_A + 1)$,
$$
E[v] = E[u+z >0] = 0.5,
$$
and we get the same estimate for variance. In fact, as long as the linear predictor only consists of Gaussian terms with sum of mean 0, we should get the same. Maybe we can try to include the 1/4 factor in the denominator when computing heritability? 